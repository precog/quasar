/*
 * Copyright 2014â€“2017 SlamData Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package quasar.physical.sparkcore.fs

import slamdata.Predef._
import quasar.Data
import quasar.contrib.pathy._
import quasar.effect._
import quasar.fp.free._
import quasar.fp.numeric.{Natural, Positive}
import quasar.fs._, FileSystemError._

import org.apache.spark.SparkContext
import org.apache.spark.rdd._
import scalaz._, Scalaz._
import scalaz.concurrent.Task

final case class SparkCursor(rdd: Option[RDD[(Data, Long)]], pointer: Int)

object readfile {

  type Offset = Natural
  type Limit  = Option[Positive]

  final case class Input[S[_]](
      rddFrom: (AFile, Offset, Limit) => Free[S, RDD[(Data, Long)]],
      fileExists: AFile => Free[S, Boolean],
      readChunkSize: () => Int
  )

  import ReadFile.ReadHandle

  def chrooted[S[_]](input: Input[S], prefix: ADir)(
      implicit
      s0: KeyValueStore[ReadHandle, SparkCursor, ?] :<: S,
      s1: Read[SparkContext, ?] :<: S,
      s2: MonotonicSeq :<: S,
      s3: Task :<: S): ReadFile ~> Free[S, ?] =
    flatMapSNT(interpret(input)) compose chroot.readFile[ReadFile](prefix)

  def interpret[S[_]](input: Input[S])(
      implicit
      s0: KeyValueStore[ReadHandle, SparkCursor, ?] :<: S,
      s1: Read[SparkContext, ?] :<: S,
      s2: MonotonicSeq :<: S,
      s3: Task :<: S): ReadFile ~> Free[S, ?] =
    new (ReadFile ~> Free[S, ?]) {
      def apply[A](rf: ReadFile[A]) = rf match {
        case ReadFile.Open(f, offset, limit) => open[S](f, offset, limit, input)
        case ReadFile.Read(h)                => read[S](h, input.readChunkSize())
        case ReadFile.Close(h)               => close[S](h)
      }
    }

  private def open[S[_]](f: AFile, offset: Offset, limit: Limit, input: Input[S])(
      implicit
      kvs: KeyValueStore.Ops[ReadHandle, SparkCursor, S],
      s1: Read[SparkContext, ?] :<: S,
      gen: MonotonicSeq.Ops[S]): Free[S, FileSystemError \/ ReadHandle] = {

    def freshHandle: Free[S, ReadHandle] =
      gen.next map (ReadHandle(f, _))

    def _open: Free[S, ReadHandle] =
      for {
        rdd <- input.rddFrom(f, offset, limit)
        cur = SparkCursor(rdd.some, 0)
        h <- freshHandle
        _ <- kvs.put(h, cur)
      } yield h

    def _empty: Free[S, ReadHandle] =
      for {
        h <- freshHandle
        cur = SparkCursor(None, 0)
        _ <- kvs.put(h, cur)
      } yield h

    input
      .fileExists(f)
      .ifM(
        _open map (_.right[FileSystemError]),
        _empty map (_.right[FileSystemError])
      )
  }

  private def read[S[_]](h: ReadHandle, step: Int)(
      implicit
      kvs: KeyValueStore.Ops[ReadHandle, SparkCursor, S],
      s1: Task :<: S): Free[S, FileSystemError \/ Vector[Data]] = {

    kvs
      .get(h)
      .toRight(unknownReadHandle(h))
      .flatMap {
        case SparkCursor(None, _) =>
          Vector.empty[Data].pure[EitherT[Free[S, ?], FileSystemError, ?]]
        case SparkCursor(Some(rdd), p) =>
          val collect = lift(Task.delay {
            rdd
              .filter(d => d._2 >= p && d._2 < (p + step))
              .map(_._1)
              .collect
              .toVector
          }).into[S].liftM[FileSystemErrT]

          for {
            collected <- collect
            cur = if (collected.isEmpty) SparkCursor(None, 0)
            else SparkCursor(some(rdd), p + step)
            _ <- kvs.put(h, cur).liftM[FileSystemErrT]
          } yield collected

      }
      .run
  }

  private def close[S[_]](h: ReadHandle)(
      implicit
      kvs: KeyValueStore.Ops[ReadHandle, SparkCursor, S]): Free[S, Unit] = kvs.delete(h)
}
